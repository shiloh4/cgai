<head>
    <script>
      window.MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']],
          displayMath: [['$$', '$$'], ['\\[', '\\]']]
        }
      };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>
  </head>
  
<style>
    /* Custom styles for headings */
    .h1-custom {
        font-size: 2.25rem; /* equivalent to text-4xl */
        font-weight: 700; /* equivalent to font-bold */
        margin-bottom: 1.5rem; /* equivalent to mb-6 */
    }

    .h2-custom {
        font-size: 1.5rem; /* equivalent to text-2xl */
        font-weight: 600; /* equivalent to font-semibold */
        margin-top: 1.5rem; /* equivalent to mt-6 */
        margin-bottom: 1rem; /* equivalent to mb-4 */
    }

    .h3-custom {
        font-size: 1.25rem; /* equivalent to text-xl */
        font-weight: 500; /* equivalent to font-medium */
        margin-top: 1rem; /* equivalent to mt-4 */
        margin-bottom: 0.5rem; /* equivalent to mb-2 */
    }

    .h4-custom {
        font-size: 1.05rem; /* equivalent to text-xl */
        font-weight: 500; /* equivalent to font-medium */
        margin-top: 1rem; /* equivalent to mt-4 */
        margin-bottom: 0.5rem; /* equivalent to mb-2 */
    }

    /* Styling for code elements */
    .code-inline {
        font-family: 'Fira Code', Consolas, 'Courier New', monospace;
        font-size: 1.1rem;
        font-weight: bold;
    }

    .link-inline {
        text-decoration: underline;  /* Ensures the links are underlined */
        color: #1d4ed8;              /* Sets the blue color for the links */
        font-family: 'Fira Code', Consolas, 'Courier New', monospace;
        font-size: 1.1rem;
        font-weight: bold;
    }

    /* Fixing the problem with ul and li elements */
    ul {
        list-style-type: disc;  /* Ensures the default bullet style */
        margin-left: 2rem;      /* Adds left indentation for the list */
        padding-left: 1.5rem;   /* Adds extra padding inside the list */
    }

    li {
        margin-bottom: 0.5rem; /* Adds space between list items */
    }

    .image-container {
        display: flex; 
        justify-content: center; /* Center images */
        gap: 20px; /* Space between images */
    }
    .image-container img {
        width: 50%; /* Adjust width */
        height: auto; /* Maintain aspect ratio */
    }

</style>

<h1 class="h1-custom">
    Assignment 2B: Neural Radiance Fields (NeRF)
</h1>

<p class="mb-4">
    Welcome to the second part of our exploration of implicit neural representations! In this section, you will work with Neural Radiance Fields (NeRF), a method for novel view synthesis that uses a neural network to represent volumetric scenes. This assignment consists of two main tasks: first, you will implement and train a NeRF model in Python using a Colab Jupyter Notebook. Then, you will render the trained model using WebGL. Letâ€™s start our creation!
</p>

<h2 class="h2-custom">Reading</h2>
<p class="mb-4">
    Before diving into coding, refer to our course slides and supplementary reading materials to understand NeRF and volume rendering. Here is the reading list:
</p>
<ul class="list-disc pl-8 mb-4">
    <li>Course Slides on NeRF</li>
    <li><a href="https://www.matthewtancik.com/nerf" class="link-inline">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</a></li>
    <li><a href="https://arxiv.org/pdf/2209.02417" class="link-inline">Volume Rendering Digest (for NeRF)</a></li>
    <li><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html" class="link-inline">PyTorch Official Document (First Four Sections)</a></li>
    <li><a href="https://cgai-gatech.vercel.app/tutorial/colab-tutorial" class="link-inline">CGAI Tutorial: Google Colab</a></li>
    <li><a href="https://cgai-gatech.vercel.app/tutorial/pytorch-tutorial" class="link-inline">CGAI Tutorial: Training a Network in PyTorch</a></li>
</ul>

<h2 class="h2-custom">Starter Code</h2>
<p class="mb-4">
    Please visit our GitHub repository for the starter code: <a href="https://github.com/cg-gatech/cgai" class="link-inline">https://github.com/cg-gatech/cgai</a>. 
    The assignment code is located in <code class="code-inline">src/app/(assignment)/assignment/A2b</code>. It contains:
</p>
<ul class="list-disc pl-8 mb-4">
    <li><code class="code-inline">nerf.ipynb</code>: Jupyter notebook for training</li>
    <li><code class="code-inline">fragment.glsl</code>: GLSL shader for rendering</li>
    <li><code class="code-inline">tiny_lego.npz</code>: Dataset for tiny lego</li>
    <li><code class="code-inline">tiny_chair.npz</code>: Dataset for tiny chair</li>
    <li><code class="code-inline">tiny_hotdog.npz</code>: Dataset for tiny hotdog</li>
    <li><code class="code-inline">tiny_drum.npz</code>: Dataset for tiny drum</li>
    <li><code class="code-inline">hotdog.txt</code>: A given hotdog NeRF model</li>
</ul>

<h2 class="h2-custom">Requirements</h2>
<p class="mb-4">
    This assignment consists of two main components: implementing a simplified NeRF model and rendering it in a GLSL-based WebGL environment. You may choose to either first train the NeRF by following the substeps in Step I and then visualize it in the GLSL shader in Step II, or complete the rendering functions in Step II first, verify them using the given neural hotdog model provided (<code>hotdog.txt</code>), and then return to implement Step I. The two steps are briefly described as follows.
</p>

<h3 class="h3-custom">Step 1: NeRF Model Implementation</h3>
<p class="mb-4">
    You are asked to complete the following four functions for a simple NeRF model. Refer to the step-by-step instructions given in the notebook for a detailed guidance.
</p>
<h3 class="h4-custom">Step 1.1: Positional Encoding</h3>
<p class="mb-4">
    To enable the network to learn high-frequency details, NeRF applies positional encoding to 3D input coordinates. This transformation extends each coordinate into a higher-dimensional space by applying a series of sine and cosine functions (refer to Page 42 of the lecture slides). Note that we additionally include the input p itself, following the original implementation of NeRF.
</p>
<div class="image-container">
    <img src="/assignments/A2b_img/posenc.png" alt="Positional Encoding">
</div>
<h3 class="h4-custom">Step 1.2: Volume Rendering</h3>
<p class="mb-4">
    NeRF generates images using volume rendering, which involves integrating density and color values along rays cast from the camera. The core steps include partitioning the range between the <code>near</code> and <code>far</code> planes into <code>N_samples</code> evenly spaced bins, sampling depth values from each bin (refer to Page 35 of the lecture slides), computing 3D sample positions along rays, applying positional encoding, and passing the encoded features through the NeRF network to predict color and density.
</p>

<p class="mb-4">
    You are required to implement the remaining steps as the following:
</p>

<ul class="list-disc pl-8 mb-4">
    <li>Apply a Sigmoid activation to color to constrain it within [0,1], and use ReLU on density to ensure non-negative values.</li>
    <li>Compute distances between adjacent depth samples, and apply the volume rendering equation (refer to Page 36 of the lecture slides):</li>
    <div class="image-container">
        <img src="/assignments/A2b_img/formula_rendering_fixed.png" alt="Volume rendering equation">
    </div>
    <li>Compute final pixel color by summing contributions from each sample along the ray.</li>
</ul>


<h3 class="h4-custom">Step 1.3: Network Training</h3>
<p class="mb-4">
    During training, NeRF learns to synthesize novel views by minimizing the difference between rendered and ground truth images. The training loop consists of:
</p>
<ul class="list-disc pl-8 mb-4">
    <li>Generating ray origins and directions from camera parameters. Note that we have implemented this for you in the NeRF class.</li>
    <li>Rendering an image by performing volume rendering (the one you implemented in 1.2) along each ray.</li>
    <li>Computing the Mean Squared Error (MSE) loss against the ground truth image.</li>
    <li>Performing backpropagation and updating model weights using an optimizer.</li>
</ul>

<h3 class="h4-custom">Step 1.4: Testing and Evaluation</h3>
<p class="mb-4">
    After training, evaluate NeRF by rendering test images from test viewpoints. Compute the Peak Signal-to-Noise Ratio (PSNR) to measure image quality:
</p>
<div class="image-container">
    <img src="/assignments/A2b_img/formula_psnr_smaller2.png" alt="PSNR Formula">
</div>
<p class="mb-4">
    Compare rendered results with ground truth images and analyze performance.
</p>
<h3 class="h4-custom">Step 1.5: Expected Output</h3>
<p class="mb-4">
    If you have correctly implemented Step 1.1-1.4, you should be able to obtain a result similar to the following image:
</p>

<div class="image-container">
    <img src="/assignments/A2b_img/testlego.png" alt="Rendered NeRF Output">
</div>


<h3 class="h3-custom">Step 2: Rendering with GLSL</h3>
<p class="mb-4">
    After training your NeRF model, serialize your model by running the final cell of the notebook, as you did in A1b, and copy the learned weights to the function <code>queryNetwork()</code> in the shader file <code>fragment.glsl</code> and implement the volume rendering algorithm in WebGL. You may reuse most of your A1a implementation code. However, you should now use NeRF to predict the rgb value and the density value for a specific point. You need to use the Sigmoid and ReLU functions as what you did in your pytorch code.
</p>

<p class="mb-4">
    A default hotdog NeRF is given to you in <code>hotdog.txt</code>, therefore, if you have correctly implemented Step 2 and directly run your fragment.glsl with the function <code>queryNetwork()</code> copied from <code>hotdog.txt</code>, you should get the top left result in the video below. Once you have implemented Step 1 together with Step 2, you should be able to obtain the video for the lego model (the right bottom one in the following video). Note that the video below with composited four object is only for demonstration purposes, in reality, you can only render one NeRF object at one time.
</p>
    
<video controls autoplay loop muted>
    <source src="/assignments/a2b-ref.mp4" type="video/mp4">
    Your browser does not support the video tag.
</video>

<h2 class="h2-custom">Creative Expression</h2>
<p class="mb-4">
    In the Creative Expression section of this assignment, you are encouraged to train your customized NeRF model. You may either use one of the datasets provided in our starter code, or you may download new datasets, e.g., from <a href="https://www.kaggle.com/datasets/nguyenhung1903/nerf-synthetic-dataset" class="link-inline">Nerf Synthetic Dataset</a> and train their NeRF representations in Colab. Note that you should convert their datasets into a compatible form that your pytorch code can use. For example, the dataset should have three keys "images", "poses" and "focal". Refer to the <code>NeRFDataset</code> class in the notebook for more details. Furthermore, you may adjust some hyperparameters (e.g., the number of frequencies of positional encoding, the learning rate, etc.) to improve the appearance of your NeRF model. However, NeRF is inherently not a small model, and this implementation has already been significantly simplified to ensure successful shader compilation and enable real-time rendering. <strong>Therefore, we do not recommend modifying the network structure such as adding hidden layers or increasing the number of hidden features because it may lead to shader compilation failure and require adjustments to the final serialization function.</strong> For example, increasing the hidden layer size from 64 (the default) to 128 can cause shader compilation failure. The creative expression theme for this assignment is <strong>Radiant Reality</strong>.
</p>

<h2 class="h2-custom">Submission</h2>
<ul class="list-disc pl-8 mb-4">
    <li>Source code for <code>nerf.ipynb</code> and <code>fragment.glsl.</code></li>
    <li>Your default rendered video for the lego model.</li>
    <li>Your output image from Step 1.5.</li>
    <li>Your customized video with different datasets for creative expression.</li>
    <li>A concise technical explanation of your implementation.</li>
</ul>

<h2 class="h2-custom">Grading</h2>
<p>This assignment is worth a total of 8 points, with the grading criteria outlined as follows:</p>
<ul class="list-disc pl-8 mb-4">
    <li>
        <strong>Technical Contribution (7 points):</strong>
        <ul class="list-disc pl-8 mb-4">
            <li>NeRF Model Implementation: 5 points (one point for each substep)</li>
            <li>GLSL Rendering: 2 points</li>
        </ul>
    </li>
    <li>
        <strong>Creative Expression (1 point):</strong> This aspect focuses on your ability to create new NeRF objects.
    </li>
</ul>


<h2 class="h2-custom">Sharing Your Work</h2>
<p>You are encouraged to share your graphical work with the class. If you want to do so, please upload your image to the Ed Discussion post <strong>A2B Gallery: Radiant Reality</strong>. This is an excellent opportunity to engage with your peers and gain recognition for your work. Share with us the beauty of realism of your radiance field!</p>